@article{Angermueller2016,
abstract = {Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.},
author = {Angermueller, Christof and P{\"{a}}rnamaa, Tanel and Parts, Leopold and Oliver, Stegle},
doi = {10.15252/msb.20156651},
file = {:Users/nickpawlowski/Documents/Mendeley/Angermueller et al. - 2016 - Deep Learning for Computational Biology.pdf:pdf},
issn = {1744-4292},
journal = {Molecular Systems Biology},
keywords = {15252,20156651,accepted 6 june 2016,cellular imaging,computational biology,deep learning,doi 10,learning,machine,msb,received 11 april 2016,regulatory genomics,revised 2 june 2016},
number = {12},
pages = {878},
title = {{Deep Learning for Computational Biology}},
year = {2016}
}
@misc{caicedo_profiling,
abstract = {A dramatic shift has occurred in how biologists use microscopy images. Whether experiments are small-scale or high-throughput, automatically quantifying biological properties in images is now widespread. We see yet another revolution under way: a transition towards using automated image analysis to not only identify phenotypes a biologist specifically seeks to measure ('screening') but also as an unbiased and sensitive tool to capture a wide variety of subtle features of cell (or organism) state ('profiling'). Mapping similarities among samples using image-based (morphological) profiling has tremendous potential to transform drug discovery, functional genomics, and basic biological research. Applications include target identification, lead hopping, library enrichment, functionally annotating genes/alleles, and identifying small molecule modulators of gene activity and disease-specific phenotypes.},
author = {Caicedo, Juan C and Singh, Shantanu and Carpenter, Anne E},
booktitle = {Current Opinion in Biotechnology},
doi = {10.1016/j.copbio.2016.04.003},
file = {:Users/nickpawlowski/Documents/Mendeley/Caicedo, Singh, Carpenter - 2016 - Applications in image-based profiling of perturbations(2).pdf:pdf},
issn = {18790429},
pages = {134--142},
pmid = {27089218},
title = {{Applications in image-based profiling of perturbations}},
volume = {39},
year = {2016}
}
@article{Caie2010,
abstract = {The application of high-content imaging in conjunction with multivariate clustering techniques has recently shown value in the confirmation of cellular activity and further characterization of drug mode of action following pharmacologic perturbation. However, such practical examples of phenotypic profiling of drug response published to date have largely been restricted to cell lines and phenotypic response markers that are amenable to basic cellular imaging. As such, these approaches preclude the analysis of both complex heterogeneous phenotypic responses and subtle changes in cell morphology across physiologically relevant cell panels. Here, we describe the application of a cell-based assay and custom designed image analysis algorithms designed to monitor morphologic phenotypic response in detail across distinct cancer cell types. We further describe the integration of these methods with automated data analysis workflows incorporating principal component analysis, Kohonen neural networking, and kNN classification to enable rapid and robust interrogation of such data sets. We show the utility of these approaches by providing novel insight into pharmacologic response across four cancer cell types, Ovcar3, MiaPaCa2, and MCF7 cells wild-type and mutant for p53. These methods have the potential to drive the development of a new generation of novel therapeutic classes encompassing pharmacologic compositions or polypharmacology in appropriate disease context.},
author = {Caie, Peter D and Walls, Rebecca E and Ingleston-Orme, Alexandra and Daya, Sandeep and Houslay, Tom and Eagle, Rob and Roberts, Mark E and Carragher, Neil O},
doi = {10.1158/1535-7163.MCT-09-1148},
file = {:Users/nickpawlowski/Documents/Mendeley/Caie et al. - 2010 - High-content phenotypic profiling of drug response signatures across distinct cancer cells.pdf:pdf},
isbn = {1538-8514 (Electronic)$\backslash$r1535-7163 (Linking)},
issn = {1535-7163},
journal = {Molecular cancer therapeutics},
keywords = {10,1158,1535-7163,2010,doi,lished onlinefirst june 8,mct-09-1148},
number = {6},
pages = {1913--1926},
pmid = {20530715},
title = {{High-content phenotypic profiling of drug response signatures across distinct cancer cells.}},
volume = {9},
year = {2010}
}
@article{Carpenter2006,
abstract = {Biologists can now prepare and image thousands of samples per day using automation, enabling chemical screens and functional genomics (for example, using RNA interference). Here we describe the first free, open-source system designed for flexible, high-throughput cell image analysis, CellProfiler. CellProfiler can address a variety of biological questions quantitatively, including standard assays (for example, cell count, size, per-cell protein levels) and complex morphological assays (for example, cell/organelle shape or subcellular patterns of DNA or protein staining).},
archivePrefix = {arXiv},
arxivId = {arXiv:1201.3109v1},
author = {Carpenter, Anne E and Jones, Thouis R and Lamprecht, Michael R and Clarke, Colin and Kang, In Han and Friman, Ola and Guertin, David a and Chang, Joo Han and Lindquist, Robert a and Moffat, Jason and Golland, Polina and Sabatini, David M},
doi = {10.1186/gb-2006-7-10-r100},
eprint = {arXiv:1201.3109v1},
file = {:Users/nickpawlowski/Documents/Mendeley/Carpenter et al. - 2006 - CellProfiler image analysis software for identifying and quantifying cell phenotypes.pdf:pdf},
isbn = {1465-6914 (Electronic)},
issn = {1465-6914},
journal = {Genome biology},
number = {10},
pages = {R100},
pmid = {17076895},
title = {{CellProfiler: image analysis software for identifying and quantifying cell phenotypes.}},
volume = {7},
year = {2006}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:Users/nickpawlowski/Documents/Mendeley/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
volume = {7},
year = {2015}
}
@article{Kandaswamy2016,
author = {Kandaswamy, C. and Silva, L. M. and Alexandre, L. A. and Santos, J. M.},
doi = {10.1177/1087057115623451},
file = {:Users/nickpawlowski/Documents/Mendeley/Kandaswamy et al. - 2016 - High-Content Analysis of Breast Cancer Using Single-Cell Deep Transfer Learning.pdf:pdf},
issn = {1087-0571},
journal = {Journal of Biomolecular Screening},
keywords = {cancer drug discovery,deep transfer learning,high-content screening,image analysis},
pmid = {26746583},
title = {{High-Content Analysis of Breast Cancer Using Single-Cell Deep Transfer Learning}},
url = {http://jbx.sagepub.com/cgi/doi/10.1177/1087057115623451},
year = {2016}
}
@article{Kraus2016,
abstract = {High Content Screening (HCS) technologies that combine automated fluorescence microscopy with high throughput biotechnology have become powerful systems for studying cell biology and drug screening. These systems can produce more than 100 000 images per day, making their success dependent on automated image analysis. In this review, we describe the steps involved in quantifying microscopy images and different approaches for each step. Typically, individual cells are segmented from the background using a segmentation algorithm. Each cell is then quantified by extracting numerical features, such as area and intensity measurements. As these feature representations are typically high dimensional ({\textgreater}500), modern machine learning algorithms are used to classify, cluster and visualize cells in HCS experiments. Machine learning algorithms that learn feature representations, in addition to the classification or clustering task, have recently advanced the state of the art on several benchmarking tasks in the computer vision community. These techniques have also recently been applied to HCS image analysis.},
author = {Kraus, Oren Z and Frey, Brendan J},
doi = {10.3109/10409238.2015.1135868},
file = {:Users/nickpawlowski/Documents/Mendeley/Kraus, Frey - 2016 - Computer vision for high content screening.pdf:pdf},
issn = {1549-7798},
journal = {Critical reviews in biochemistry and molecular biology},
keywords = {Cells,classification,deep learning,high content screening,machine learning,microscopy,segmentation},
number = {January},
pages = {1--8},
title = {{Computer vision for high content screening.}},
url = {http://www.tandfonline.com/doi/abs/10.3109/10409238.2015.1135868?journalCode=ibmg20},
volume = {9238},
year = {2016}
}
@article{Kraus2016a,
abstract = {Convolutional neural networks (CNN) have achieved state of the art performance on both classification and segmentation tasks. Applying CNNs to microscopy images is challenging due to the lack of datasets labeled at the single cell level. We extend the application of CNNs to microscopy image classification and segmentation using multiple instance learning (MIL). We present the adaptive Noisy-AND MIL pooling function, a new MIL operator that is robust to outliers. Combining CNNs with MIL enables training CNNs using full resolution microscopy images with global labels. We base our approach on the similarity between the aggregation function used in MIL and pooling layers used in CNNs. We show that training MIL CNNs end-to-end outperforms several previous methods on both mammalian and yeast microscopy images without requiring any segmentation steps.},
author = {Kraus, Oren Z. and Ba, Jimmy Lei and Frey, Brendan J.},
doi = {10.1093/bioinformatics/btw252},
file = {:Users/nickpawlowski/Documents/Mendeley/Kraus, Ba, Frey - 2016 - Classifying and segmenting microscopy images with deep multiple instance learning.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
number = {12},
pages = {i52--i59},
pmid = {27307644},
title = {{Classifying and segmenting microscopy images with deep multiple instance learning}},
volume = {32},
year = {2016}
}
@article{Ljosa2013,
author = {Ljosa, V. and Caie, P. D. and ter Horst, R. and Sokolnicki, K. L. and Jenkins, E. L. and Daya, S. and Roberts, M. E. and Jones, T. R. and Singh, S. and Genovesio, A. and Clemons, P. A. and Carragher, N. O. and Carpenter, A. E.},
doi = {10.1177/1087057113503553},
file = {:Users/nickpawlowski/Documents/Mendeley/Ljosa et al. - 2013 - Comparison of Methods for Image-Based Profiling of Cellular Morphological Responses to Small-Molecule Treatment(2).pdf:pdf},
issn = {1087-0571},
journal = {Journal of Biomolecular Screening},
keywords = {drug profiling,high-content screening,image-based screening,phenotypic screening},
number = {10},
pages = {1321--1329},
title = {{Comparison of Methods for Image-Based Profiling of Cellular Morphological Responses to Small-Molecule Treatment}},
url = {http://jbx.sagepub.com/cgi/doi/10.1177/1087057113503553},
volume = {18},
year = {2013}
}
@article{Ljosa2012,
abstract = {... Metrics for: Annotated  high - throughput  microscopy  image  sets for validation . ... $\backslash$n},
author = {Ljosa, Vebjorn and Sokolnicki, Katherine L and Carpenter, Anne E},
doi = {10.1038/nmeth.2083},
file = {:Users/nickpawlowski/Documents/Mendeley/Ljosa, Sokolnicki, Carpenter - 2012 - Annotated high-throughput microscopy image sets for validation.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature Methods},
number = {7},
pages = {637--637},
pmid = {22743765},
title = {{Annotated high-throughput microscopy image sets for validation}},
volume = {9},
year = {2012}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/nickpawlowski/Documents/Mendeley/Simonyan, Zisserman - 2015 - V d c n l -s i r.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Singh2014,
abstract = {The presence of systematic noise in images in high-throughput microscopy experiments can significantly impact the accuracy of downstream results. Among the most common sources of systematic noise is non-homogeneous illumination across the image field. This often adds an unacceptable level of noise, obscures true quantitative differences and precludes biological experiments that rely on accurate fluorescence intensity measurements. In this paper, we seek to quantify the improvement in the quality of high-content screen readouts due to software-based illumination correction. We present a straightforward illumination correction pipeline that has been used by our group across many experiments. We test the pipeline on real-world high-throughput image sets and evaluate the performance of the pipeline at two levels: (a) Z'-factor to evaluate the effect of the image correction on a univariate readout, representative of a typical high-content screen, and (b) classification accuracy on phenotypic signatures derived from the images, representative of an experiment involving more complex data mining. We find that applying the proposed post-hoc correction method improves performance in both experiments, even when illumination correction has already been applied using software associated with the instrument. To facilitate the ready application and future development of illumination correction methods, we have made our complete test data sets as well as open-source image analysis pipelines publicly available. This software-based solution has the potential to improve outcomes for a wide-variety of image-based HTS experiments.},
author = {Singh, S. and Bray, M. A. and Jones, T. R. and Carpenter, A. E.},
doi = {10.1111/jmi.12178},
file = {:Users/nickpawlowski/Documents/Mendeley/Singh et al. - 2014 - Pipeline for illumination correction of images for high-throughput microscopy.pdf:pdf},
isbn = {1617714895},
issn = {13652818},
journal = {Journal of Microscopy},
keywords = {Fluorescence microscopy,High-throughput microscopy,Illumination correction,Shading correction,Vignetting},
number = {3},
pages = {231--236},
pmid = {25228240},
title = {{Pipeline for illumination correction of images for high-throughput microscopy}},
volume = {256},
year = {2014}
}
@article{Sommer2013,
abstract = {Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline.},
author = {Sommer, Christoph and Gerlich, Daniel W},
doi = {10.1242/jcs.123604},
file = {:Users/nickpawlowski/Documents/Mendeley/Sommer, Gerlich - 2013 - Machine learning in cell biology – teaching computers to recognize phenotypes.pdf:pdf},
isbn = {1477-9137 (Electronic)$\backslash$r0021-9533 (Linking)},
issn = {1477-9137},
journal = {Journal of Cell Sciience},
keywords = {bioimage informatics,computer vision,high-content screening,machine learning,microscopy},
number = {Pt 24},
pages = {5529--5539},
pmid = {24259662},
title = {{Machine learning in cell biology – teaching computers to recognize phenotypes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24259662},
volume = {126},
year = {2013}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at rel-atively low computational cost. Recently, the introduction of residual connections in conjunction with a more tradi-tional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Incep-tion networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These varia-tions improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We fur-ther demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
eprint = {1602.07261},
file = {:Users/nickpawlowski/Documents/Mendeley/Szegedy, Ioffe, Vanhoucke - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
journal = {Arxiv},
pages = {12},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
year = {2016}
}
@article{Szegedy2014,
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and Hill, Chapel and Arbor, Ann},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/nickpawlowski/Documents/Mendeley/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pmid = {24920543},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error and 17.3{\%} top-1 error.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1002/2014GB005021},
eprint = {1512.00567},
file = {:Users/nickpawlowski/Documents/Mendeley/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {9781617796029},
issn = {08866236},
journal = {arXiv preprint},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Y.2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Y., Lecun and Y., Bengio and G., Hinton},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:Users/nickpawlowski/Documents/Mendeley/Y., Y., G. - 2015 - Deep learning.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84930630277{\&}partnerID=40{\&}md5=befeefa64ddca265c713cf81f4e2fc54},
volume = {521},
year = {2015}
}
@article{Zamparo2015,
abstract = {High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap.},
archivePrefix = {arXiv},
arxivId = {1501.0134},
author = {Zamparo, Lee and Zhang, Zhaolei},
eprint = {1501.0134},
file = {:Users/nickpawlowski/Documents/Mendeley/Zamparo, Zhang - 2015 - Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data.pdf:pdf},
pages = {5},
title = {{Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data}},
url = {http://arxiv.org/abs/1501.01348},
year = {2015}
}
