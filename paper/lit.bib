@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@article{radford2015unsupervised,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015}
}

@article{mao2016least,
  title={Least squares generative adversarial networks},
  author={Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond YK and Wang, Zhen and Smolley, Stephen Paul},
  journal={arXiv preprint ArXiv:1611.04076},
  year={2016}
}

@article{arjovsky2017wasserstein,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}

@article{donahue2016adversarial,
  title={Adversarial feature learning},
  author={Donahue, Jeff and Kr{\"a}henb{\"u}hl, Philipp and Darrell, Trevor},
  journal={arXiv preprint arXiv:1605.09782},
  year={2016}
}

@MastersThesis{pawlowski2016msc,
    author     =     {Nick Pawlowski},
    title     =     {{Towards Image-Based Morphological Profiling using Deep Learning Techniques}},
    school     =     {University of Edinburgh},
    year     =     {2016},
    url = {doc.ic.ac.uk/~np716/mscthesis.pdf}
    }

@article{Angermueller2016,
abstract = {Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.},
author = {Angermueller, Christof and P{\"{a}}rnamaa, Tanel and Parts, Leopold and Oliver, Stegle},
doi = {10.15252/msb.20156651},
file = {:Users/nickpawlowski/Documents/Mendeley/Angermueller et al. - 2016 - Deep Learning for Computational Biology.pdf:pdf},
issn = {1744-4292},
journal = {Molecular Systems Biology},
keywords = {15252,20156651,accepted 6 june 2016,cellular imaging,computational biology,deep learning,doi 10,learning,machine,msb,received 11 april 2016,regulatory genomics,revised 2 june 2016},
number = {12},
pages = {878},
title = {{Deep Learning for Computational Biology}},
year = {2016}
}
@article{caicedo_profiling,
  title={Applications in image-based profiling of perturbations},
  author={Caicedo, Juan C and Singh, Shantanu and Carpenter, Anne E},
  journal={Current Opinion in Biotechnology},
  volume={39},
  pages={134--142},
  year={2016},
  publisher={Elsevier}
}
@article{Caie2010,
abstract = {The application of high-content imaging in conjunction with multivariate clustering techniques has recently shown value in the confirmation of cellular activity and further characterization of drug mode of action following pharmacologic perturbation. However, such practical examples of phenotypic profiling of drug response published to date have largely been restricted to cell lines and phenotypic response markers that are amenable to basic cellular imaging. As such, these approaches preclude the analysis of both complex heterogeneous phenotypic responses and subtle changes in cell morphology across physiologically relevant cell panels. Here, we describe the application of a cell-based assay and custom designed image analysis algorithms designed to monitor morphologic phenotypic response in detail across distinct cancer cell types. We further describe the integration of these methods with automated data analysis workflows incorporating principal component analysis, Kohonen neural networking, and kNN classification to enable rapid and robust interrogation of such data sets. We show the utility of these approaches by providing novel insight into pharmacologic response across four cancer cell types, Ovcar3, MiaPaCa2, and MCF7 cells wild-type and mutant for p53. These methods have the potential to drive the development of a new generation of novel therapeutic classes encompassing pharmacologic compositions or polypharmacology in appropriate disease context.},
author = {Caie, Peter D and Walls, Rebecca E and Ingleston-Orme, Alexandra and Daya, Sandeep and Houslay, Tom and Eagle, Rob and Roberts, Mark E and Carragher, Neil O},
doi = {10.1158/1535-7163.MCT-09-1148},
file = {:Users/nickpawlowski/Documents/Mendeley/Caie et al. - 2010 - High-content phenotypic profiling of drug response signatures across distinct cancer cells.pdf:pdf},
isbn = {1538-8514 (Electronic)$\backslash$r1535-7163 (Linking)},
issn = {1535-7163},
journal = {Molecular Cancer Therapeutics},
keywords = {10,1158,1535-7163,2010,doi,lished onlinefirst june 8,mct-09-1148},
number = {6},
pages = {1913--1926},
pmid = {20530715},
title = {{High-content phenotypic profiling of drug response signatures across distinct cancer cells.}},
volume = {9},
year = {2010}
}
@article{Carpenter2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1201.3109v1},
author = {Carpenter, Anne E and Jones, Thouis R and Lamprecht, Michael R and Clarke, Colin and Kang, In Han and Friman, Ola and Guertin, David a and Chang, Joo Han and Lindquist, Robert a and Moffat, Jason and Golland, Polina and Sabatini, David M},
doi = {10.1186/gb-2006-7-10-r100},
eprint = {arXiv:1201.3109v1},
isbn = {1465-6914 (Electronic)},
issn = {1465-6914},
journal = {Genome biology},
number = {10},
pages = {R100},
pmid = {17076895},
title = {{{CellProfiler: image analysis software for identifying and quantifying cell phenotypes.}}},
volume = {7},
year = {2006}
}
@article{He2015,
        author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
        title = {Deep Residual Learning for Image Recognition},
        journal = {arXiv preprint arXiv:1512.03385},
        year = {2015}
    }
@article{Kandaswamy2016,
author = {Kandaswamy, C. and Silva, L. M. and Alexandre, L. A. and Santos, J. M.},
doi = {10.1177/1087057115623451},
file = {:Users/nickpawlowski/Documents/Mendeley/Kandaswamy et al. - 2016 - High-Content Analysis of Breast Cancer Using Single-Cell Deep Transfer Learning.pdf:pdf},
issn = {1087-0571},
journal = {Journal of Biomolecular Screening},
keywords = {cancer drug discovery,deep transfer learning,high-content screening,image analysis},
pmid = {26746583},
title = {{High-Content Analysis of Breast Cancer Using Single-Cell Deep Transfer Learning}},
url = {http://jbx.sagepub.com/cgi/doi/10.1177/1087057115623451},
year = {2016}
}
@article{Kraus2016,
abstract = {High Content Screening (HCS) technologies that combine automated fluorescence microscopy with high throughput biotechnology have become powerful systems for studying cell biology and drug screening. These systems can produce more than 100 000 images per day, making their success dependent on automated image analysis. In this review, we describe the steps involved in quantifying microscopy images and different approaches for each step. Typically, individual cells are segmented from the background using a segmentation algorithm. Each cell is then quantified by extracting numerical features, such as area and intensity measurements. As these feature representations are typically high dimensional ({\textgreater}500), modern machine learning algorithms are used to classify, cluster and visualize cells in HCS experiments. Machine learning algorithms that learn feature representations, in addition to the classification or clustering task, have recently advanced the state of the art on several benchmarking tasks in the computer vision community. These techniques have also recently been applied to HCS image analysis.},
author = {Kraus, Oren Z and Frey, Brendan J},
doi = {10.3109/10409238.2015.1135868},
file = {:Users/nickpawlowski/Documents/Mendeley/Kraus, Frey - 2016 - Computer vision for high content screening.pdf:pdf},
issn = {1549-7798},
journal = {Critical Reviews in Biochemistry and Molecular Biology},
keywords = {Cells,classification,deep learning,high content screening,machine learning,microscopy,segmentation},
number = {January},
pages = {1--8},
title = {{Computer vision for high content screening.}},
url = {http://www.tandfonline.com/doi/abs/10.3109/10409238.2015.1135868?journalCode=ibmg20},
volume = {9238},
year = {2016}
}
@article{Kraus2016a,
abstract = {Convolutional neural networks (CNN) have achieved state of the art performance on both classification and segmentation tasks. Applying CNNs to microscopy images is challenging due to the lack of datasets labeled at the single cell level. We extend the application of CNNs to microscopy image classification and segmentation using multiple instance learning (MIL). We present the adaptive Noisy-AND MIL pooling function, a new MIL operator that is robust to outliers. Combining CNNs with MIL enables training CNNs using full resolution microscopy images with global labels. We base our approach on the similarity between the aggregation function used in MIL and pooling layers used in CNNs. We show that training MIL CNNs end-to-end outperforms several previous methods on both mammalian and yeast microscopy images without requiring any segmentation steps.},
author = {Kraus, Oren Z. and Ba, Jimmy Lei and Frey, Brendan J.},
doi = {10.1093/bioinformatics/btw252},
file = {:Users/nickpawlowski/Documents/Mendeley/Kraus, Ba, Frey - 2016 - Classifying and segmenting microscopy images with deep multiple instance learning.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
number = {12},
pages = {i52--i59},
pmid = {27307644},
title = {{Classifying and segmenting microscopy images with deep multiple instance learning}},
volume = {32},
year = {2016}
}
@article{Ljosa2013,
author = {Ljosa, V. and Caie, P. D. and ter Horst, R. and Sokolnicki, K. L. and Jenkins, E. L. and Daya, S. and Roberts, M. E. and Jones, T. R. and Singh, S. and Genovesio, A. and Clemons, P. A. and Carragher, N. O. and Carpenter, A. E.},
doi = {10.1177/1087057113503553},
file = {:Users/nickpawlowski/Documents/Mendeley/Ljosa et al. - 2013 - Comparison of Methods for Image-Based Profiling of Cellular Morphological Responses to Small-Molecule Treatment(2).pdf:pdf},
issn = {1087-0571},
journal = {Journal of Biomolecular Screening},
keywords = {drug profiling,high-content screening,image-based screening,phenotypic screening},
number = {10},
pages = {1321--1329},
title = {{Comparison of Methods for Image-Based Profiling of Cellular Morphological Responses to Small-Molecule Treatment}},
url = {http://jbx.sagepub.com/cgi/doi/10.1177/1087057113503553},
volume = {18},
year = {2013}
}
@article{Ljosa2012,
abstract = {... Metrics for: Annotated  high - throughput  microscopy  image  sets for validation . ... $\backslash$n},
author = {Ljosa, Vebjorn and Sokolnicki, Katherine L and Carpenter, Anne E},
doi = {10.1038/nmeth.2083},
file = {:Users/nickpawlowski/Documents/Mendeley/Ljosa, Sokolnicki, Carpenter - 2012 - Annotated high-throughput microscopy image sets for validation.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature Methods},
number = {7},
pages = {637--637},
pmid = {22743765},
title = {{Annotated high-throughput microscopy image sets for validation}},
volume = {9},
year = {2012}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/nickpawlowski/Documents/Mendeley/Simonyan, Zisserman - 2015 - V d c n l -s i r.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Singh2014,
abstract = {The presence of systematic noise in images in high-throughput microscopy experiments can significantly impact the accuracy of downstream results. Among the most common sources of systematic noise is non-homogeneous illumination across the image field. This often adds an unacceptable level of noise, obscures true quantitative differences and precludes biological experiments that rely on accurate fluorescence intensity measurements. In this paper, we seek to quantify the improvement in the quality of high-content screen readouts due to software-based illumination correction. We present a straightforward illumination correction pipeline that has been used by our group across many experiments. We test the pipeline on real-world high-throughput image sets and evaluate the performance of the pipeline at two levels: (a) Z'-factor to evaluate the effect of the image correction on a univariate readout, representative of a typical high-content screen, and (b) classification accuracy on phenotypic signatures derived from the images, representative of an experiment involving more complex data mining. We find that applying the proposed post-hoc correction method improves performance in both experiments, even when illumination correction has already been applied using software associated with the instrument. To facilitate the ready application and future development of illumination correction methods, we have made our complete test data sets as well as open-source image analysis pipelines publicly available. This software-based solution has the potential to improve outcomes for a wide-variety of image-based HTS experiments.},
author = {Singh, S. and Bray, M. A. and Jones, T. R. and Carpenter, A. E.},
doi = {10.1111/jmi.12178},
file = {:Users/nickpawlowski/Documents/Mendeley/Singh et al. - 2014 - Pipeline for illumination correction of images for high-throughput microscopy.pdf:pdf},
isbn = {1617714895},
issn = {13652818},
journal = {Journal of Microscopy},
keywords = {Fluorescence microscopy,High-throughput microscopy,Illumination correction,Shading correction,Vignetting},
number = {3},
pages = {231--236},
pmid = {25228240},
title = {{Pipeline for illumination correction of images for high-throughput microscopy}},
volume = {256},
year = {2014}
}
@article{Sommer2013,
abstract = {Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline.},
author = {Sommer, Christoph and Gerlich, Daniel W},
doi = {10.1242/jcs.123604},
file = {:Users/nickpawlowski/Documents/Mendeley/Sommer, Gerlich - 2013 - Machine learning in cell biology – teaching computers to recognize phenotypes.pdf:pdf},
isbn = {1477-9137 (Electronic)$\backslash$r0021-9533 (Linking)},
issn = {1477-9137},
journal = {Journal of Cell Sciience},
keywords = {bioimage informatics,computer vision,high-content screening,machine learning,microscopy},
number = {Pt 24},
pages = {5529--5539},
pmid = {24259662},
title = {{Machine learning in cell biology – teaching computers to recognize phenotypes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24259662},
volume = {126},
year = {2013}
}
@article{pawlowski2016automating,
  title={Automating Morphological Profiling with Generic Deep Convolutional Networks},
  author={Pawlowski, Nick and Caicedo, Juan C and Singh, Shantanu and Carpenter, Anne E and Storkey, Amos},
  journal={NIPS 2016 Workshop on Machine Learning in Computational Biology},
  year={2016},
}

@article{ando2017improving,
  title={Improving Phenotypic Measurements in High-Content Imaging Screens},
  author={Ando, D Michael and McLean, Cory and Berndl, Marc},
  journal={bioRxiv},
  pages={161422},
  year={2017},
  publisher={Cold Spring Harbor Laboratory}
}

@article{caicedo2017data,
  title={Data-analysis strategies for image-based cell profiling},
  author={Caicedo, Juan C and Cooper, Sam and Heigwer, Florian and Warchal, Scott and Qiu, Peng and Molnar, Csaba and Vasilevich, Aliaksei S and Barry, Joseph D and Bansal, Harmanjit Singh and Kraus, Oren and others},
  journal={Nature methods},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{osokin2017gans,
  title={GANs for Biological Image Synthesis},
  author={Osokin, Anton and Chessel, Anatole and Salas, Rafael E Carazo and Vaggi, Federico},
  journal={arXiv preprint arXiv:1708.04692},
  year={2017}
}

@article{Szegedy2016,
  title={Inception-v4, inception-ResNet and the Impact of Residual Connections on Learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
  journal={arXiv preprint arXiv:1602.07261},
  year={2016}
}
@article{Szegedy2014,
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and Hill, Chapel and Arbor, Ann},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/nickpawlowski/Documents/Mendeley/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pmid = {24920543},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@article{Szegedy2015,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  journal={arXiv preprint arXiv:1512.00567},
  year={2015}
}
@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{Zamparo2015,
  title={{Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data}},
  author={Zamparo, Lee and Zhang, Zhaolei},
  journal={arXiv preprint arXiv:1501.01348},
  year={2015}
}
@article{johnson2017generative,
  title={Generative Modeling with Conditional Autoencoders: Building an Integrated Cell},
  author={Johnson, Gregory R and Donovan-Maiye, Rory M and Maleckar, Mary M},
  journal={arXiv preprint arXiv:1705.00092},
  year={2017}
}